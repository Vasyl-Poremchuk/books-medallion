{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql import DataFrame"
   ],
   "id": "d4cc52e3b80eb5d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BASE_S3_URI = \"s3://book-scraping-data\"\n",
    "PROCESSED_S3_URI = f\"{BASE_S3_URI}/processed\"\n",
    "\n",
    "CATALOG = \"catalog\"\n",
    "SCHEMA = \"bronze\"\n",
    "\n",
    "DATA_CONFIG = [\n",
    "    {\n",
    "        \"source_filename\": \"popular_lists.parquet.gz\",\n",
    "        \"dest_table\": \"raw_popular_list\",\n",
    "    },\n",
    "    {\"source_filename\": \"books.parquet.gz\", \"dest_table\": \"raw_book\"},\n",
    "    {\n",
    "        \"source_filename\": \"book_details.parquet.gz\",\n",
    "        \"dest_table\": \"raw_book_details\",\n",
    "    },\n",
    "]"
   ],
   "id": "9ef5c4a80060a1c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_checkpoint_path(*, base_s3_uri: str, dest_table: str) -> str:\n",
    "    \"\"\"Create a checkpoint path where the inferred schema and subsequent\n",
    "    table changes will be stored.\n",
    "\n",
    "    :param base_s3_uri: The base URI of the S3 bucket where the checkpoint\n",
    "        should be located.\n",
    "    :param dest_table: The destination table name.\n",
    "    :return: Checkpoint path of the destination table.\n",
    "    \"\"\"\n",
    "    checkpoint_path = f\"{base_s3_uri}/_checkpoint/{dest_table}\"\n",
    "\n",
    "    return checkpoint_path\n",
    "\n",
    "\n",
    "def create_dest_path(*, catalog: str, schema: str, dest_table: str) -> str:\n",
    "    \"\"\"Create a destination path where the data should be written.\n",
    "\n",
    "    :param catalog: The catalog name.\n",
    "    :param schema: The schema name.\n",
    "    :param dest_table: The destination table name.\n",
    "    :return: Destination path of the table.\n",
    "    \"\"\"\n",
    "    dest_path = f\"{catalog}.{schema}.{dest_table}\"\n",
    "\n",
    "    return dest_path\n",
    "\n",
    "\n",
    "def add_collected_at(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Add the data of collection of the data extracted from\n",
    "    the source path.\n",
    "\n",
    "    :param df: The dataframe to which the column should be added.\n",
    "    :return: Dataframe with the added column.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\n",
    "        \"collected_at\", sf.get(sf.split(\"source_path\", pattern=\"/\"), index=4)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"collected_at\", sf.to_date(\"collected_at\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def ingest_to_table(*, source_filename: str, dest_table: str) -> None:\n",
    "    \"\"\"Ingest source data into destination table.\n",
    "\n",
    "    :param source_filename: The source filename.\n",
    "    :param dest_table: The destination table name.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    checkpoint_path = create_checkpoint_path(\n",
    "        base_s3_uri=BASE_S3_URI, dest_table=dest_table\n",
    "    )\n",
    "    dest_path = create_dest_path(\n",
    "        catalog=CATALOG, schema=SCHEMA, dest_table=dest_table\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "        .option(\"pathGlobFilter\", source_filename)\n",
    "        .load(PROCESSED_S3_URI)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"ingested_at\", sf.current_timestamp())\n",
    "    df = df.withColumn(\"source_path\", sf.col(\"_metadata.file_path\"))\n",
    "\n",
    "    df = add_collected_at(df=df)\n",
    "\n",
    "    (\n",
    "        df.writeStream.format(\"delta\")\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(dest_path)\n",
    "    )"
   ],
   "id": "66d9f85bfd28608b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for data_config in DATA_CONFIG:\n",
    "  ingest_to_table(**data_config)"
   ],
   "id": "be47a3f8c5c40498"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
